{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-5bcd2d099962>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01municodedata\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m WordNetLemmatizer\n\u001b[0;32m---> 15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtokenization\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import pylab\n",
    "import emoji\n",
    "import re\n",
    "import contractions\n",
    "from bs4 import BeautifulSoup\n",
    "import unicodedata\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config IPCompleter.greedy=True\n",
    "%matplotlib inline\n",
    "%load_ext nb_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "curr_dir = \"..\"\n",
    "train_file = os.path.join(curr_dir, \"dataset\", \"train.csv\")\n",
    "test_file = os.path.join(curr_dir, \"dataset\", \"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"legend.fontsize\": \"x-large\",\n",
    "    \"figure.figsize\": (16, 5),\n",
    "    \"axes.labelsize\": \"x-large\",\n",
    "    \"axes.titlesize\": \"x-large\",\n",
    "    \"xtick.labelsize\": \"x-large\",\n",
    "    \"ytick.labelsize\": \"x-large\",\n",
    "}\n",
    "pylab.rcParams.update(params)\n",
    "sns.set_theme(style=\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(train_file)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotBarGraph(data, labelx, labely, title, switch_axis=False):\n",
    "    y = data.index.tolist()\n",
    "    x = data.tolist()\n",
    "    if switch_axis:\n",
    "        x = data.index.tolist()\n",
    "        y = data.tolist()\n",
    "    sns.barplot(y=y, x=x)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(labelx)\n",
    "    plt.ylabel(labely)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "locations_map = df.location.value_counts().sort_values(ascending=False)[:10]\n",
    "plotBarGraph(locations_map, \"Frequency\", \"Countries\", \"Tweet locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Disaster related Keywords\n",
    "keywords_map = (\n",
    "    df.keyword.loc[df.target == 1].value_counts().sort_values(ascending=False)[:20]\n",
    ")\n",
    "plotBarGraph(keywords_map, \"Frequency\", \"Keywords\", \"Top 20 disaster related keywords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 20 Non-Disaster related Keywords\n",
    "keywords_non_disaster_map = df.keyword.loc[df.target == 0].value_counts(\n",
    "    ascending=False\n",
    ")[:20]\n",
    "plotBarGraph(\n",
    "    keywords_non_disaster_map,\n",
    "    \"Frequency\",\n",
    "    \"Keywords\",\n",
    "    \"Top 20 non-disaster related keywords\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"In the graphs(Top 20 disaster keywords and non-disaster keywords) above\\n we see that disaster keywords are natural or artificial calamilites.\\nWhile non-disaster keywords are too generalized keywords and not sufficient to describe a disaster.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.countplot(x=\"target\", data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing Duplicate Text Data\n",
    "text_data_duplicate_map = df.text.duplicated().value_counts()\n",
    "print(text_data_duplicate_map)\n",
    "plotBarGraph(\n",
    "    text_data_duplicate_map,\n",
    "    \"Duplicate\",\n",
    "    \"Frequency\",\n",
    "    \"Text Data Duplicate or Not\",\n",
    "    switch_axis=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original dataframe shape: \", df.shape)\n",
    "df.drop_duplicates(subset=\"text\", keep=\"first\", inplace=True)\n",
    "print(\"Dropping duplicate rows and keeping original value shape: \", df.shape)\n",
    "df.text.duplicated().value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_text_data(query, column=\"text\"):\n",
    "    return df[df[column].str.contains(query)][column]\n",
    "\n",
    "\n",
    "print(search_text_data(\"volcano\")[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower case text data\n",
    "def lower_case_data(data=\"\"):\n",
    "    data = data.lower()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handle Emojis\n",
    "def sentences_with_emojis(id_texts):\n",
    "    sentences = []\n",
    "    indeces = id_texts[0]\n",
    "    texts = id_texts[1]\n",
    "    for index, sentence in zip(indeces, texts):\n",
    "        has_emoji = bool(emoji.get_emoji_regexp().search(sentence))\n",
    "        if has_emoji:\n",
    "            sentences.append((index, sentence))\n",
    "    if len(sentences) == 0:\n",
    "        return \"Sentences are clean and don't have emojis!\"\n",
    "    else:\n",
    "        return sentences\n",
    "\n",
    "\n",
    "# Source: https://stackoverflow.com/questions/33404752/removing-emojis-from-a-string-in-python\n",
    "def clean_emojis(text):\n",
    "    allchars = [str for str in text]\n",
    "    emoji_list = [c for c in allchars if c in emoji.UNICODE_EMOJI]\n",
    "    clean_text = \" \".join(\n",
    "        [str for str in text.split() if not any(i in str for i in emoji_list)]\n",
    "    )\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean urls\n",
    "def clean_urls(text):\n",
    "    text = re.sub(r\"https?://\\S+\", \"\", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all sorts of special characters and punctuations.\n",
    "def removeSpecialChar(text):\n",
    "    sentence = []\n",
    "    for s in text:\n",
    "        if s == \" \":\n",
    "            sentence.append(s)\n",
    "        if s.isalnum():\n",
    "            sentence.append(s)\n",
    "    return \"\".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check any html text in df.text (not cleaned yet) column\n",
    "def checkHtml(text):\n",
    "    return bool(BeautifulSoup(text, \"html.parser\").find())\n",
    "\n",
    "\n",
    "html_sentence_map = df[\"text\"].apply(checkHtml).tolist()\n",
    "if not any(html_sentence_map):\n",
    "    print(\"No text containing html found!\")\n",
    "else:\n",
    "    print(\"There is some html text!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove accented text\n",
    "def remove_accented_chars(text):\n",
    "    new_text = (\n",
    "        unicodedata.normalize(\"NFKD\", text)\n",
    "        .encode(\"ascii\", \"ignore\")\n",
    "        .decode(\"utf-8\", \"ignore\")\n",
    "    )\n",
    "    return new_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explore Stopwords list from nltk\n",
    "def remove_words_having_not(words=[\"\"]):\n",
    "    for word in words:\n",
    "        if \"not\" in word:\n",
    "            words.remove(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "stopwords_list = stopwords.words(\"english\")\n",
    "print(stopwords_list)\n",
    "stopwords_list = remove_words_having_not(\n",
    "    [removeSpecialChar(contractions.fix(word)) for word in stopwords_list]\n",
    ")\n",
    "print(\"\\n---------------Stopwords after preprocessing---------------\\n\")\n",
    "print(stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords from nltk corpus\n",
    "def remove_stopwords(text):\n",
    "    new_sentence = \"\"\n",
    "    for word in text.split():\n",
    "        if word not in stopwords_list:\n",
    "            new_sentence += word + \" \"\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove numbers from text\n",
    "def remove_numbers(text=\"\"):\n",
    "    new_sentence = \"\"\n",
    "    for word in text.split():\n",
    "        num_free_word = \"\".join([i for i in word if not i.isdigit()])\n",
    "        new_sentence += num_free_word + \" \"\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmatization\n",
    "def lemmatize(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    new_sentence = \"\"\n",
    "    for word in text.split():\n",
    "        lematized_word = lemmatizer.lemmatize(word)\n",
    "        new_sentence += lematized_word + \" \"\n",
    "    return new_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final data cleaning step is removing non-essential whitespaces\n",
    "def remove_white_space(text):\n",
    "    return \" \".join(text.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text cleaning pipeline\n",
    "def clean_sentence_pipeline(text):\n",
    "    text = lower_case_data(text)\n",
    "    text = clean_emojis(text)\n",
    "    text = clean_urls(text)\n",
    "    text = contractions.fix(text)\n",
    "    text = removeSpecialChar(text)\n",
    "    text = remove_accented_chars(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = remove_numbers(text)\n",
    "    text = lemmatize(text)\n",
    "    return remove_white_space(text)\n",
    "\n",
    "\n",
    "def clean_text_column(col_name):\n",
    "    df[\"clean_text\"] = \"\"\n",
    "    df[\"clean_text\"] = df[col_name].apply(clean_sentence_pipeline)\n",
    "\n",
    "\n",
    "clean_text_column(\"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparison of cleaned texts\n",
    "df[[\"text\", \"clean_text\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert Model Implementation\n",
    "'''\n",
    "BERT: Bidirectional Encoder Representations from Transformers\n",
    "\n",
    "Vocab:\n",
    "a. Masked Language Model: Mask random words in a sentence(fill in the blanks) \n",
    "    to understand bidirectional context.\n",
    "b. Next sentence prediction: Two sentences and detect which sentences follows each other.\n",
    "\n",
    "Process:\n",
    "Token inputs -> BERT Transformer Encoders -> Output as next sentence prediction and mask language modeling\n",
    "-> Pass to sigmoid for classification.\n",
    "\n",
    "1. Token Inputs\n",
    "To generate token inputs, we need token embeddings(WordPiece Embedding) + Segment Embeddings(distinguish sentences)\n",
    "+ Position Embeddings(position of word within a sentence encoded as a vector).\n",
    "\n",
    "token embeddings: [CLS] token in the beginning of the sentence and [SEP] token at the end of the sentence\n",
    "segment embeddings: distinguish sentences with tokens assigned to each sentence\n",
    "position embedding: unique tokens assigned to each word in the sentence.\n",
    " \n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "Embedding space=> Map words in sentence to a word cluster(embedding space)(Glove) to generate a vector for the word\n",
    "Positional Encoder=> Context of the word with respect to the sentence vector\n",
    "Embedding space + Positional encoding = encoding of word with context= EC\n",
    "\n",
    "Encoder Block:feed input (\"The red dog\")\n",
    "EC -> Multi-headed attention layer -> Feed forward layer\n",
    "\n",
    "Attention layer: How relavent is the ith word  with respect to the other words in the same sentence\n",
    "Feed forward nets: pass attention vectors into a simple neural net\n",
    "\n",
    "Decoder Block: feed output (\"le red chein\")\n",
    "Take the EC of output\n",
    "EC -> multi-headed attention -> multi headed attention(encoded-decoded attention mapping(English to french translation happens here)\n",
    "-> Feed foward layer -> linear -> softmax \n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zsh:1: command not found: wget\r\n"
     ]
    }
   ],
   "source": [
    "# !wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert config\n",
    "bert_layer = hub.KerasLayer(\"https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/1\",trainable=True)\n",
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
